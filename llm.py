import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import fitz
from docx import Document
import os

class LLM:
    def __init__(self, model_id: str, device="cuda:0"):
        """Kh·ªüi t·∫°o m√¥ h√¨nh LLM v·ªõi tokenizer v√† model t·ª´ Hugging Face."""
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device)

    def generate_response(self, user_input, max_new_tokens=4096, temperature=0.7, top_k=50, top_p=0.95):
        """Sinh ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh d·ª±a tr√™n tin nh·∫Øn ƒë·∫ßu v√†o."""
        messages = [
            {"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† th√¢n thi·ªán, s·∫µn s√†ng h·ªó tr·ª£ ng∆∞·ªùi d√πng."},
            {"role": "user", "content": user_input}
        ]

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)
        inputs = self.tokenizer(prompt, return_tensors="pt")
        inputs = {key: value.to(self.model.device) for key, value in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs["input_ids"],
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
            )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        last_assistant_index = generated_text.rfind('assistant\n\n')
        
        if last_assistant_index != -1:
            filtered_content = generated_text[last_assistant_index + len('assistant\n\n'):].strip()
        else:
            filtered_content = "Kh√¥ng t√¨m th·∫•y n·ªôi dung c·∫ßn thi·∫øt."
        
        del inputs
        del outputs
        torch.cuda.empty_cache()

        return filtered_content

    def extract_text_from_pdf(self, file_path):
        """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ t·ªáp PDF."""
        doc = fitz.open(file_path)
        text = "\n".join([page.get_text("text") for page in doc])
        return text

    def summarize_file(self, file, max_size_mb=1, max_new_tokens=4096, temperature=0.7, top_k=50, top_p=0.95):
        """T√≥m t·∫Øt n·ªôi dung t·ª´ file vƒÉn b·∫£n (PDF, DOCX, TXT)."""

        # Ki·ªÉm tra k√≠ch th∆∞·ªõc file
        file_size = os.path.getsize(file.name) / (1024 * 1024)
        if file_size > max_size_mb:
            return f"‚ö†Ô∏è K√≠ch th∆∞·ªõc file qu√° l·ªõn ({file_size:.2f} MB). Gi·ªõi h·∫°n l√† {max_size_mb} MB."

        # Ki·ªÉm tra ƒë·ªãnh d·∫°ng file
        allowed_extensions = [".pdf", ".docx", ".txt"]
        file_extension = os.path.splitext(file.name)[-1].lower()
        if file_extension not in allowed_extensions:
            return f"‚ö†Ô∏è ƒê·ªãnh d·∫°ng file kh√¥ng h·ª£p l·ªá ({file_extension}). Vui l√≤ng t·∫£i l√™n file PDF, DOCX ho·∫∑c TXT."


        if file.name.endswith(".docx"):
            doc = Document(file.name)
            content = "\n".join([para.text for para in doc.paragraphs])
        elif file.name.endswith(".pdf"):
            content = self.extract_text_from_pdf(file.name)
        else:
            with open(file.name, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

        messages = [
            {"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t√≥m t·∫Øt t√†i li·ªáu."},
            {"role": "user", "content": f"""
             
            üìå Y√™u c·∫ßu:

            Ti√™u ƒë·ªÅ n·ªôi dung: ƒê∆∞a ra m·ªôt ti√™u ƒë·ªÅ ch√≠nh cho t√†i li·ªáu.
            T√≥m t·∫Øt theo ph·∫ßn: Chia t√≥m t·∫Øt th√†nh c√°c m·ª•c t∆∞∆°ng ·ª©ng v·ªõi c·∫•u tr√∫c n·ªôi dung.
            L√†m n·ªïi b·∫≠t th√¥ng tin quan tr·ªçng: S·ª≠ d·ª•ng bullet points ho·∫∑c danh s√°ch ƒë√°nh s·ªë n·∫øu c·∫ßn.

            üéØ ƒê·∫ßu ra mong mu·ªën:

            Ti√™u ƒë·ªÅ: [T√™n t√†i li·ªáu]

            1. Gi·ªõi thi·ªáu: [T√≥m t·∫Øt ph·∫ßn gi·ªõi thi·ªáu]
            2. N·ªôi dung ch√≠nh: [T√≥m t·∫Øt c√°c ƒëi·ªÉm quan tr·ªçng]
            3. K·∫øt lu·∫≠n: [T√≥m t·∫Øt k·∫øt lu·∫≠n]
            H√£y t√≥m t·∫Øt t√†i li·ªáu sau b·∫±ng c√°ch chia nh·ªè th√†nh c√°c ph·∫ßn quan tr·ªçng:
            
             
            D∆∞·ªõi ƒë√¢y l√† n·ªôi dung c·ªßa t·ªáp:
            {content}
            """}
        ]
        
        return self.generate_response(messages, max_new_tokens, temperature, top_k, top_p)
